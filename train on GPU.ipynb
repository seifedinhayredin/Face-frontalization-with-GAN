{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd70f645-30d3-4811-974a-d90ff2e40686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b4d1421-7464-4dc0-a9b4-64a3b75edd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def is_jpeg(filename):\n",
    "  return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\"])\n",
    "\n",
    "def get_subdirs(directory):\n",
    "  subdirs = sorted([os.path.join(directory, name) for name in sorted(os.listdir(directory)) if os.path.isdir(os.path.join(directory, name))])\n",
    "  return subdirs\n",
    "\n",
    "class ExternalInputIterator:\n",
    "  def __init__(self, imageset_dir, batch_size, random_shuffle=False):\n",
    "    self.imageset_dir = imageset_dir\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    # Get subdirectories (assuming \"pose\" and \"frontal\" folders exist)\n",
    "    self.pose_dirs = os.path.join(imageset_dir, \"pose\")\n",
    "    self.frontal_dir = os.path.join(imageset_dir, \"frontal\")\n",
    "\n",
    "    # Collect profile image paths\n",
    "    self.profile_files = [os.path.join(self.pose_dirs, file) for file in sorted(os.listdir(self.pose_dirs)) if is_jpeg(file)]\n",
    "\n",
    "    # Collect frontal image paths\n",
    "    self.frontal_files = [os.path.join(self.frontal_dir, file) for file in sorted(os.listdir(self.frontal_dir)) if is_jpeg(file)]\n",
    "\n",
    "    # Shuffle if necessary\n",
    "    if random_shuffle:\n",
    "      np.random.shuffle(self.profile_files)\n",
    "      np.random.shuffle(self.frontal_files)\n",
    "\n",
    "    self.i = 0\n",
    "    self.n = len(self.profile_files)\n",
    "\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    profiles = []\n",
    "    frontals = []\n",
    "\n",
    "    for _ in range(self.batch_size):\n",
    "      profile_filename = self.profile_files[self.i]\n",
    "      frontal_filename = self.match_frontal_image(profile_filename)\n",
    "\n",
    "      with Image.open(profile_filename) as profile_img:\n",
    "        profile_tensor = torch.from_numpy(np.array(profile_img)).float().to(device)  # Move to GPU\n",
    "        profiles.append(profile_tensor)\n",
    "      with Image.open(frontal_filename) as frontal_img:\n",
    "        frontal_tensor = torch.from_numpy(np.array(frontal_img)).float().to(device)  # Move to GPU\n",
    "        frontals.append(frontal_tensor)\n",
    "\n",
    "      self.i = (self.i + 1) % self.n\n",
    "\n",
    "    return (profiles, frontals)\n",
    "\n",
    "  def match_frontal_image(self, profile_filename):\n",
    "    profile_name = os.path.basename(profile_filename).split(\"_\")[0]\n",
    "    for frontal_file in self.frontal_files:\n",
    "      if profile_name in frontal_file:\n",
    "        return frontal_file\n",
    "    return None\n",
    "\n",
    "class ImagePipeline:\n",
    "  def __init__(self, imageset_dir, image_size=128, random_shuffle=False, batch_size=64, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    self.eii = ExternalInputIterator(imageset_dir, batch_size, random_shuffle)\n",
    "    self.iterator = iter(self.eii)\n",
    "    self.num_inputs = len(self.eii.profile_files)\n",
    "    self.image_size = image_size\n",
    "    self.device = device\n",
    "\n",
    "  def epoch_size(self, name=None):\n",
    "    return self.num_inputs\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_inputs\n",
    "\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    (images, targets) = next(self.iterator)\n",
    "\n",
    "    # Resize and normalize using PyTorch tensors on GPU\n",
    "    #resized_images = torch.nn.functional.interpolate(torch.stack(images, dim=0).to(self.device), size=(self.image_size, self.image_size), mode='bilinear', align_corners=True)\n",
    "    resized_images = torch.nn.functional.interpolate(torch.stack(images, dim=0).unsqueeze(1).to(self.device), size=(self.image_size, self.image_size), mode='bilinear', align_corners=True)\n",
    "\n",
    "    #resized_targets = torch.nn.functional.interpolate(torch.stack(targets, dim=0).to(self.device), size=(self.image_size, self.image_size), mode='bilinear', align_corners=True)\n",
    "    resized_targets = torch.nn.functional.interpolate(torch.stack(targets, dim=0).unsqueeze(1).to(self.device),size=(self.image_size, self.image_size), mode='bilinear', align_corners=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Assuming you have already defined resized_images and resized_targets on GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check for GPU availability\n",
    "    \n",
    "    # Ensure tensors are on the chosen device (GPU or CPU)\n",
    "    resized_images = resized_images.to(device)\n",
    "    resized_targets = resized_targets.to(device)\n",
    "    \n",
    "    # Perform normalization on the GPU\n",
    "    normalized_images = (resized_images - 128.0) / 128.0\n",
    "    normalized_targets = (resized_targets - 128.0) / 128.0\n",
    "\n",
    "\n",
    "    # Alternatively, calculate mean and standard deviation on the fly if needed\n",
    "    # normalized_images = (resized_images - resized_images.mean(dim=[1, 2], keepdim=True)) / resized_images.std(dim=[1, 2], keepdim=True)\n",
    "    # normalized_targets = (resized_targets - resized_targets.mean(dim=[1, 2], keepdim=True)) / resized_targets.std(dim=[1, 2], keepdim=True)\n",
    "\n",
    "    return (normalized_images, normalized_targets)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Advance the iterator to the desired index\n",
    "    for _ in range(index):\n",
    "      next(self.iterator)\n",
    "\n",
    "    # Return the next batch\n",
    "    return next(self)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f40a9181-bd38-46ed-9ad1-5f96f8fee704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415436288"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d868780-8dba-4e91-9209-0f0d53847560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70b2cea6-eb6b-4832-bbb3-dd786c84630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, width, height = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, width, height)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15ec894a-aca3-40d0-974a-e7eab2c89cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool = self.avg_pool(x)\n",
    "        max_pool = self.max_pool(x)\n",
    "        avg_out = self.fc(avg_pool)\n",
    "        max_out = self.fc(max_pool)\n",
    "        out = avg_out + max_out\n",
    "        return out * x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b41999c-9dc1-43e2-9466-e736d9213ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),  # 64x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(64)  # Add channel attention\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),         # 32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(128)  # Add channel attention\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),        # 16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(256)  # Add channel attention\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),        # 8x8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(512)  # Add channel attention\n",
    "        )\n",
    "        self.encoder5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),        # 4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(512)  # Add channel attention\n",
    "        )\n",
    "        self.encoder6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),        # 2x2\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(512)  # Add channel attention\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, 4, 2, 1),       # 1x1\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1),  # 2x2\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1),   # 4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1),  # 8x8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, 4, 2, 1),   # 16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            SelfAttention(256)  # Add self-attention\n",
    "        )\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, 4, 2, 1),   # 32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            SelfAttention(128)  # Add self-attention\n",
    "        )\n",
    "        self.decoder5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 64, 4, 2, 1),    # 64x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 1, 4, 2, 1),  # 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(enc1)\n",
    "        enc3 = self.encoder3(enc2)\n",
    "        enc4 = self.encoder4(enc3)\n",
    "        enc5 = self.encoder5(enc4)\n",
    "        enc6 = self.encoder6(enc5)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc6)\n",
    "\n",
    "        # Decoding and adding skip connection\n",
    "        dec1 = self.decoder1(torch.cat([bottleneck, enc6], dim=1))\n",
    "        dec2 = self.decoder2(torch.cat([dec1, enc5], dim=1))\n",
    "        dec3 = self.decoder3(torch.cat([dec2, enc4], dim=1))\n",
    "        dec4 = self.decoder4(torch.cat([dec3, enc3], dim=1))\n",
    "        dec5 = self.decoder5(torch.cat([dec4, enc2], dim=1))\n",
    "        decoded = self.decoder6(torch.cat([dec5, enc1], dim=1))\n",
    "\n",
    "        return decoded\n",
    "\n",
    "# Example usage:\n",
    "# generator = G()\n",
    "# generator.apply(weights_init)\n",
    "# print(generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3fcc002d-5c26-4eea-b458-70412ce82ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RelativeAvgDiscriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(RelativeAvgDiscriminator, self).__init__()\n",
    "\n",
    "    # Separate feature extraction for real and generated data\n",
    "    self.conv_real = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, 4, 2, 1),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(16, 32, 4, 2, 1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(32, 64, 4, 2, 1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "    )\n",
    "    self.conv_generated = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, 4, 2, 1),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(16, 32, 4, 2, 1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(32, 64, 4, 2, 1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "    )\n",
    "\n",
    "    # Relative Average Pooling\n",
    "    self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # Remaining convolutional layers (modified for combined features)\n",
    "    self.post_pool = nn.Sequential(\n",
    "        nn.Conv2d(128, 128, 4, 2, 1),  # Input channels changed to 128\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(128, 256, 4, 2, 1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "       \n",
    "    )\n",
    "\n",
    "    # Output layer with sigmoid activation\n",
    "    self.output = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, real, fake):\n",
    "    # Extract features from real and generated data\n",
    "    real_features = self.conv_real(real)\n",
    "    generated_features = self.conv_generated(fake)\n",
    "\n",
    "    # Concatenate features before pooling\n",
    "    combined_features = torch.cat([real_features, generated_features], dim=1)\n",
    "\n",
    "    # Relative Average Pooling\n",
    "    features = self.avgpool(combined_features)\n",
    "\n",
    "    # Process features with remaining layers\n",
    "    output = self.post_pool(features)\n",
    "\n",
    "    # Probability score\n",
    "    #probability = self.output(logits)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2fd66cf-4e30-4033-b944-f98b2a0a61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(10)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(999)\n",
    "torch.cuda.manual_seed(999)\n",
    "\n",
    "# Where is your training dataset at?\n",
    "datapath = r\"C:\\Users\\zed\\Dataset\\Grayscale_Dataset\"\n",
    "\n",
    "# Assuming you have the modified ImagePipeline class from the previous responses\n",
    "train_pipe = ImagePipeline(datapath, image_size=128, random_shuffle=True, batch_size=32)\n",
    "\n",
    "# No need to call build() without DALI\n",
    "\n",
    "# Use a standard PyTorch DataLoader instead of DALIGenericIterator\n",
    "m_train = train_pipe.epoch_size()\n",
    "train_pipe_loader = DataLoader(train_pipe)\n",
    "\n",
    "# Move criterion to the GPU\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e6cfc0c-3398-452e-bee3-a2e323f3ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Define a function to calculate PSNR\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = F.mse_loss(img1, img2)\n",
    "    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "    return psnr.item()\n",
    "\n",
    "# Define a function to calculate SSIM\n",
    "# Define a function to calculate SSIM\n",
    "def calculate_ssim(img1, img2):\n",
    "    # Ensure tensors are on the same device\n",
    "    if img1.device != img2.device:\n",
    "        raise ValueError(\"Input tensors must be on the same device\")\n",
    "\n",
    "    # Calculate SSIM directly on GPU tensors\n",
    "    img1 = img1.detach().squeeze().clamp(0, 1).cpu().numpy()  # Ensure pixel values are in [0, 1] range\n",
    "    img2 = img2.detach().squeeze().clamp(0, 1).cpu().numpy()  # Ensure pixel values are in [0, 1] range\n",
    "    return ssim(img1.transpose(1, 2, 0), img2.transpose(1, 2, 0), multichannel=True, data_range=1)\n",
    "\n",
    "\n",
    "# Define lists to store PSNR and SSIM values for each epoch\n",
    "psnr_values = []\n",
    "ssim_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef6da772-e84c-4636-a0f8-3877c3a99ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Assuming you have defined your G and RelativeAvgDiscriminator models (netG and netD)\n",
    "netG = G().to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = RelativeAvgDiscriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "L1_factor = 1\n",
    "L2_factor = 1\n",
    "GAN_factor = 0.005\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999), eps=1e-8)\n",
    "\n",
    "try:\n",
    "    os.mkdir('FF_output')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir('FF_checkpoints')\n",
    "except OSError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19c3c096-5c98-467e-8679-0339fe5ccbb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in netG.parameters():\n",
    "    print(i.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fac9a535-94bc-4658-b7c5-88c975f7972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"FF_checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1dfd6689-457b-4f75-b74b-17b24eb5b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the losses\n",
    "losses_L1 = []\n",
    "losses_L2 = []\n",
    "losses_gan = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04554ef5-ca6b-4f2c-ab04-0199b3d744eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def multi_scale_pixelwise_loss(fake_images, real_images, num_scales=3):\n",
    "    loss = 0.0\n",
    "    device = fake_images.device  # Get the device from fake_images tensor\n",
    "    \n",
    "    for scale in range(num_scales):\n",
    "        # Interpolate fake and real images on the GPU\n",
    "        fake_scaled = F.interpolate(fake_images, scale_factor=1 / (2 ** scale), mode='bilinear', align_corners=False)\n",
    "        real_scaled = F.interpolate(real_images, scale_factor=1 / (2 ** scale), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Compute pixel-wise L1 loss on the GPU\n",
    "        pixel_loss = F.l1_loss(fake_scaled, real_scaled)\n",
    "        \n",
    "        # Accumulate loss\n",
    "        loss += pixel_loss / (2 ** scale)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94ca9938-c45c-49a4-a796-ea700bcbd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "multi_scale_losses = []\n",
    "\n",
    "avg_generator_losses = []\n",
    "avg_discriminator_losses = []\n",
    "avg_multi_scale_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1613170d-b7b2-4138-b732-626c5a058410",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53726eac-33f4-40e6-bb50-dad4107aa14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  69%|████████████████████████████████████████████████▋                      | 151/220 [10:19<04:43,  4.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m total_ssim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_pipe_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_pipe_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reshape and move to device\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrontal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reshape and move to device\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\seienv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\seienv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\seienv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\seienv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[62], line 116\u001b[0m, in \u001b[0;36mImagePipeline.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    114\u001b[0m   \u001b[38;5;66;03m# Advance the iterator to the desired index\u001b[39;00m\n\u001b[0;32m    115\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(index):\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m    118\u001b[0m   \u001b[38;5;66;03m# Return the next batch\u001b[39;00m\n\u001b[0;32m    119\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "Cell \u001b[1;32mIn[62], line 51\u001b[0m, in \u001b[0;36mExternalInputIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m   profiles\u001b[38;5;241m.\u001b[39mappend(profile_tensor)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(frontal_filename) \u001b[38;5;28;01mas\u001b[39;00m frontal_img:\n\u001b[1;32m---> 51\u001b[0m   frontal_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrontal_img\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Move to GPU\u001b[39;00m\n\u001b[0;32m     52\u001b[0m   frontals\u001b[38;5;241m.\u001b[39mappend(frontal_tensor)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming `device` is a CUDA device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "for epoch in range(100):  # Assuming 3 epochs for demonstration\n",
    "    \n",
    "    # Track loss values for each epoch\n",
    "    loss_L1 = 0\n",
    "    loss_L2 = 0\n",
    "    loss_gan = 0\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    \n",
    "   \n",
    "    with tqdm(total=len(train_pipe_loader), desc=f\"Epoch {epoch}\") as pbar:\n",
    "        for i, data in enumerate(train_pipe_loader, 0):\n",
    "            profile = data[0].view(32, 1, 128, 128).to(device)  # Reshape and move to device\n",
    "            frontal = data[1].view(32, 1, 128, 128).to(device)  # Reshape and move to device\n",
    "\n",
    "            # TRAINING THE DISCRIMINATOR\n",
    "            netD.zero_grad()\n",
    "            optimizerD.zero_grad()\n",
    "\n",
    "            real = Variable(frontal).type(torch.FloatTensor).to(device)\n",
    "            target = Variable(torch.ones(real.size()[0])).to(device)\n",
    "            profile = Variable(profile).type(torch.FloatTensor).to(device)\n",
    "           \n",
    "            \n",
    "            real_output = netD(real, real)  # Discriminator output for real images\n",
    "            generated = netG(profile)  # Generate images from profile\n",
    "            fake_output = netD(profile, generated.detach())  # Discriminator output for fake images\n",
    "\n",
    "            # Concatenate real and fake outputs along a new dimension\n",
    "            concatenated = torch.cat((real_output, fake_output), dim=0)\n",
    "\n",
    "            # Create labels for real and fake images\n",
    "            target_real = torch.ones_like(real_output)\n",
    "            target_fake = torch.zeros_like(fake_output)\n",
    "            targets = torch.cat((target_real, target_fake), dim=0)\n",
    "\n",
    "            # Calculate BCE loss for the concatenated outputs\n",
    "            errD = criterion(concatenated, targets.float())\n",
    "            errD.backward()\n",
    "            optimizerD.step()\n",
    "             # Accumulate discriminator loss\n",
    "            discriminator_losses.append(errD.item())\n",
    "\n",
    "            # TRAINING THE GENERATOR\n",
    "            netG.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "            generated = netG(profile)\n",
    "            output = netD(profile, generated)\n",
    "\n",
    "            # G wants to have the synthetic images be accepted by D\n",
    "            errG_GAN = criterion(output, torch.ones_like(output).float())\n",
    "\n",
    "            # Calculate L1 and L2 loss between generated and real images\n",
    "            errG_L2 = F.mse_loss(generated, frontal.float())\n",
    "            errG_L1 = multi_scale_pixelwise_loss(generated, real)  # Multi-scale pixel-wise loss\n",
    "           \n",
    "            # Total generator loss\n",
    "            errG = GAN_factor * errG_GAN + L1_factor * errG_L1 + L2_factor * errG_L2\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "             #Accumulate generator loss\n",
    "            generator_losses.append(errG.item())\n",
    "\n",
    "            #Accumulate multi-scale pixel-wise loss\n",
    "            multi_scale_losses.append(errG_L1.item())\n",
    "\n",
    "            # Update loss values\n",
    "            loss_L1 += errG_L1.item()\n",
    "            loss_L2 += errG_L2.item()\n",
    "            loss_gan += errG_GAN.item()\n",
    "\n",
    "            # Calculate PSNR for each generated image and accumulate\n",
    "            psnr = calculate_psnr(generated, frontal)\n",
    "            total_psnr += psnr\n",
    "\n",
    "            # Calculate SSIM for each generated image and accumulate\n",
    "            ssim_val = calculate_ssim(generated, frontal)\n",
    "            total_ssim += ssim_val\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "   # Append the average losses to the respective lists\n",
    "\n",
    "\n",
    "    avg_gen_loss = sum(generator_losses[epoch * len(train_pipe_loader):(epoch + 1) * len(train_pipe_loader)]) / len(train_pipe_loader)\n",
    "    avg_disc_loss = sum(discriminator_losses[epoch * len(train_pipe_loader):(epoch + 1) * len(train_pipe_loader)]) / len(train_pipe_loader)\n",
    "    avg_multi_loss = sum(multi_scale_losses[epoch * len(train_pipe_loader):(epoch + 1) * len(train_pipe_loader)]) / len(train_pipe_loader)\n",
    "\n",
    "    avg_generator_losses.append(avg_gen_loss)\n",
    "    avg_discriminator_losses.append(avg_disc_loss)\n",
    "    avg_multi_scale_losses.append(avg_multi_loss)\n",
    "    \n",
    "    \n",
    "    # Calculate average PSNR and SSIM for this epoch\n",
    "    avg_psnr = total_psnr / len(train_pipe_loader)\n",
    "    avg_ssim = total_ssim / len(train_pipe_loader)\n",
    "\n",
    "    # Append the average PSNR and SSIM for this epoch to the respective lists\n",
    "    psnr_values.append(avg_psnr)\n",
    "    ssim_values.append(avg_ssim)\n",
    "    \n",
    "    if epoch == 0:\n",
    "        print('First training epoch completed in ',(time.time() - start_time),' seconds')\n",
    "    #if epoch > 0:\n",
    "        #print(f\"Epoch: {epoch} is starting..\")\n",
    "    # reset the DALI iterator\n",
    "    #train_pipe_loader.reset()\n",
    "\n",
    "    losses_L1.append(loss_L1 / m_train)\n",
    "    losses_L2.append(loss_L2 / m_train)\n",
    "    losses_gan.append(loss_gan / m_train)\n",
    "\n",
    "     # Save checkpoint after each epoch\n",
    "    checkpoint_state = {\n",
    "      'epoch': epoch,\n",
    "      'netG_state_dict': netG.state_dict(),\n",
    "      'netD_state_dict': netD.state_dict(),\n",
    "      'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "      'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "      'loss_L1': loss_L1,\n",
    "      'loss_L2': loss_L2,\n",
    "      'loss_gan': loss_gan,\n",
    "      'psnr_values': psnr_values,\n",
    "      'ssim_values': ssim_values,\n",
    "      'losses_L1': losses_L1,\n",
    "      'losses_L2': losses_L2,\n",
    "      'losses_gan': losses_gan,\n",
    "      'discriminator_losses': discriminator_losses,\n",
    "      'generator_losses': generator_losses,\n",
    "      'multi_scale_losses': multi_scale_losses,\n",
    "      'avg_generator_losses': avg_generator_losses,\n",
    "      'avg_discriminator_losses': avg_discriminator_losses,\n",
    "      'avg_multi_scale_losses': avg_multi_scale_losses,\n",
    "    }\n",
    "    torch.save(checkpoint_state, os.path.join(checkpoint_dir, f\"checkpoint_{epoch}.pth\"))\n",
    "\n",
    "    \n",
    "\n",
    "    # Print the absolute values of three losses to screen:\n",
    "    print('[%d/30] Training absolute losses: L1 %.7f ; L2 %.7f BCE %.7f; Average PSNR: %.2f; Average SSIM: %.4f' % ((epoch + 1), loss_L1/m_train, loss_L2/m_train, loss_gan/m_train, avg_psnr, avg_ssim, ))\n",
    "\n",
    "    # Print the PSNR and SSIM on each epoch\n",
    "    #print('[%d/30] Average PSNR: %.2f, Average SSIM: %.4f' % (epoch + 1, avg_psnr, avg_ssim))\n",
    "\n",
    "    # Save the inputs, outputs, and ground truth frontals to files:\n",
    "    vutils.save_image(profile.data, 'FF_output/%03d_input.jpg' % epoch, normalize=True)\n",
    "    vutils.save_image(real.data, 'FF_output/%03d_real.jpg' % epoch, normalize=True)\n",
    "    vutils.save_image(generated.data, 'FF_output/%03d_generated.jpg' % epoch, normalize=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # Save the pre-trained Generator as well\n",
    "    torch.save(netG,'FF_output/netG_%d.pt' % epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8828041-3fe7-4395-bd1e-b4b80718a1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa67fc-30ee-4420-969e-cdd77927aecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
