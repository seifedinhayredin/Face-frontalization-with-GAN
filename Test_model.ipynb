{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70da507c-6c0b-4772-8eab-ff682ca6f056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Additional information about the CUDA device\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab48e9-0371-4297-ae99-2ad5d78385c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05bb873-7a20-4f31-a4cf-5679236afbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def is_jpeg(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\"])\n",
    "\n",
    "class ExternalInputIterator:\n",
    "    def __init__(self, imageset_dir, batch_size, random_shuffle=False):\n",
    "        self.imageset_dir = imageset_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Only process the \"pose\" folder\n",
    "        self.pose_dir = os.path.join(imageset_dir, \"pose\")\n",
    "        \n",
    "        # Collect pose image paths\n",
    "        self.pose_files = [os.path.join(self.pose_dir, file) for file in sorted(os.listdir(self.pose_dir)) if is_jpeg(file)]\n",
    "        print(f\"Number of pose images: {len(self.pose_files)}\")\n",
    "\n",
    "        # Shuffle if necessary\n",
    "        if random_shuffle:\n",
    "            np.random.shuffle(self.pose_files)\n",
    "\n",
    "        self.i = 0\n",
    "        self.n = len(self.pose_files)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        poses = []\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            pose_filename = self.pose_files[self.i]\n",
    "            with Image.open(pose_filename) as pose_img:\n",
    "                poses.append(np.array(pose_img))\n",
    "\n",
    "            self.i = (self.i + 1) % self.n\n",
    "\n",
    "        return poses\n",
    "\n",
    "class ImagePipeline:\n",
    "    def __init__(self, imageset_dir, image_size=128, random_shuffle=False, batch_size=64, device_id=0):\n",
    "        self.eii = ExternalInputIterator(imageset_dir, batch_size, random_shuffle)\n",
    "        self.iterator = iter(self.eii)\n",
    "        self.num_inputs = len(self.eii.pose_files)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def epoch_size(self, name=None):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        images = next(self.iterator)\n",
    "\n",
    "        # Perform resizing and normalization using NumPy\n",
    "        resized_images = np.array([np.array(Image.fromarray(img).resize((self.image_size, self.image_size))) for img in images])\n",
    "\n",
    "        # Normalize using mean and standard deviation\n",
    "        normalized_images = (resized_images - 128.0) / 128.0\n",
    "\n",
    "        return normalized_images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Advance the iterator to the desired index\n",
    "        for _ in range(index):\n",
    "            next(self.iterator)\n",
    "\n",
    "        # Return the next batch\n",
    "        return next(self)\n",
    "\n",
    "# Example usage:\n",
    "# imageset_dir = 'path/to/dataset'\n",
    "# batch_size = 64\n",
    "# image_pipeline = ImagePipeline(imageset_dir, batch_size=batch_size)\n",
    "# for batch in image_pipeline:\n",
    "#     # process batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46051546-ae7e-45a8-a2d0-ce559d278e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe257279-a915-42b5-addc-bc39a17405a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, width, height = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, width, height)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89ca20-da1f-485b-8c77-cda816f55a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2802f3a-678f-4f8f-aab0-586b81a37543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool = self.avg_pool(x)\n",
    "        max_pool = self.max_pool(x)\n",
    "        avg_out = self.fc(avg_pool)\n",
    "        max_out = self.fc(max_pool)\n",
    "        out = avg_out + max_out\n",
    "        return out * x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01256125-d0fd-4614-847f-2bcca128ac39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b172a9e7-e462-4048-a8c4-84a69169086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),  # 64x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(64)  # Add channel attention\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),         # 32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(128)  # Add channel attention\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),        # 16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(256)  # Add channel attention\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),        # 8x8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(512)  # Add channel attention\n",
    "        )\n",
    "        self.encoder5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),        # 4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(512)  # Add channel attention\n",
    "        )\n",
    "        self.encoder6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),        # 2x2\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            ChannelAttention(512)  # Add channel attention\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, 4, 2, 1),       # 1x1\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1),  # 2x2\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1),   # 4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1),  # 8x8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, 4, 2, 1),   # 16x16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            SelfAttention(256)  # Add self-attention\n",
    "        )\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, 4, 2, 1),   # 32x32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            SelfAttention(128)  # Add self-attention\n",
    "        )\n",
    "        self.decoder5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 64, 4, 2, 1),    # 64x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 1, 4, 2, 1),  # 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(enc1)\n",
    "        enc3 = self.encoder3(enc2)\n",
    "        enc4 = self.encoder4(enc3)\n",
    "        enc5 = self.encoder5(enc4)\n",
    "        enc6 = self.encoder6(enc5)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc6)\n",
    "\n",
    "        # Decoding and adding skip connection\n",
    "        dec1 = self.decoder1(torch.cat([bottleneck, enc6], dim=1))\n",
    "        dec2 = self.decoder2(torch.cat([dec1, enc5], dim=1))\n",
    "        dec3 = self.decoder3(torch.cat([dec2, enc4], dim=1))\n",
    "        dec4 = self.decoder4(torch.cat([dec3, enc3], dim=1))\n",
    "        dec5 = self.decoder5(torch.cat([dec4, enc2], dim=1))\n",
    "        decoded = self.decoder6(torch.cat([dec5, enc1], dim=1))\n",
    "\n",
    "        return decoded\n",
    "\n",
    "# Example usage:\n",
    "# generator = G()\n",
    "# generator.apply(weights_init)\n",
    "# print(generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082f957-bd93-4a2c-a21c-83090e023a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e10f1a7b-e84c-405b-9800-f0b90468017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pose images: 4\n",
      "Frontalizing image 1\n",
      "Frontalizing image 2\n",
      "Frontalizing image 3\n",
      "Frontalizing image 4\n",
      "4 image is frontalized\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "random.seed(10)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(999)\n",
    "torch.cuda.manual_seed(999)\n",
    "\n",
    "#from data import ImagePipeline  # Assuming this custom class still exists\n",
    "\n",
    "device = 'cuda'\n",
    "#datapath = r'C:\\Users\\zed\\Dataset\\Mult_test'\n",
    "datapath = r\"D:\\CAS_TEST\"\n",
    "\n",
    "# Generate frontal images from the test set\n",
    "def frontalize(model, datapath, mtest):\n",
    "    \n",
    "    test_pipe = ImagePipeline(datapath, image_size=128, random_shuffle=False,batch_size = 6)  # Removed batch_size\n",
    "    test_pipe_loader = DataLoader(test_pipe, batch_size=mtest)  # Use DataLoader\n",
    "    numb_front = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in test_pipe_loader:\n",
    "             \n",
    "            profile = data[0].to(device).type(torch.float)  # Correct syntax to change data type\n",
    "            #print(profile.shape)\n",
    "            profile = profile.view(6, 1, 128, 128) \n",
    "            #print(profile.shape)\n",
    "            generated = model(profile).type(torch.float)  # Convert output to float\n",
    "\n",
    "            #profile = data[0].to(device)  # Assuming profiles are in data['profiles']\n",
    "            #print(\"length:\",len(profile))\n",
    "            #generated = model(profile)\n",
    "            vutils.save_image(torch.cat((profile, generated.data), dim = 0), 'D:/FFRAD_CAS_TEST/test.jpg', nrow=6, padding=2, normalize=True)  # Removed frontal for consistency\n",
    "            numb_front = numb_front+1\n",
    "            print(f\"Frontalizing image {numb_front}\")\n",
    "    print(f\"{numb_front} image is frontalized\")\n",
    "# Load a pre-trained Pytorch model\n",
    "saved_model = torch.load(\"D:/CAS_FF_output/netG_33.pt\")\n",
    "\n",
    "frontalize(saved_model, datapath, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568e2fc-838e-46a3-bf00-dc54c307a9af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
