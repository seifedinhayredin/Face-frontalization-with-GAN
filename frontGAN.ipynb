{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d16bf0a-4908-4ca8-b764-9262406894d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ec456-12bd-4368-a2f8-aa4b97145010",
   "metadata": {},
   "source": [
    "Data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a510a29e-a6b9-41fa-bd8a-465c8372aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from random import shuffle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image  # Import PIL for image decoding\n",
    "\n",
    "\n",
    "def is_jpeg(filename):\n",
    "    #return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\"])\n",
    "    return any(filename.endswith(extension) for extension in [\".png\"])\n",
    "\n",
    "\n",
    "def get_subdirs(directory):\n",
    "    subdirs = sorted([join(directory, name) for name in sorted(os.listdir(directory)) if os.path.isdir(os.path.join(directory, name))])\n",
    "    return subdirs\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "class ExternalInputIterator:\n",
    "    def __init__(self, imageset_dir, batch_size, random_shuffle=False):\n",
    "        self.images_dir = imageset_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # ... (Rest of the initialization code remains the same)\n",
    "    \n",
    "        # First, figure out what are the inputs and what are the targets in your directory structure:\n",
    "        # Get a list of filenames for the target (frontal) images\n",
    "        self.frontals = np.array([join(imageset_dir, frontal_file) for frontal_file in sorted(os.listdir(imageset_dir)) if is_jpeg(frontal_file)])\n",
    "        print(len(self.frontals ))\n",
    "        \n",
    "        # Get a list of lists of filenames for the input (profile) images for each person\n",
    "        profile_files = [[join(person_dir, profile_file) for profile_file in sorted(os.listdir(person_dir)) if is_jpeg(profile_file)] for person_dir in get_subdirs(imageset_dir)]\n",
    "       \n",
    "        print(len(profile_files))\n",
    "        # Build a flat list of frontal indices, corresponding to the *flattened* profile_files\n",
    "        # The reason we are doing it this way is that we need to keep track of the multiple inputs corresponding to each target\n",
    "        frontal_ind = []\n",
    "        for ind, profiles in enumerate(profile_files):\n",
    "            frontal_ind += [ind]*len(profiles)\n",
    "        self.frontal_indices = np.array(frontal_ind)\n",
    "        \n",
    "        # Now that we have built frontal_indices, we can flatten profile_files\n",
    "        self.profiles = np.array(flatten(profile_files))\n",
    "\n",
    "        # Shuffle the (input, target) pairs if necessary: in practice, it is profiles and frontal_indices that get shuffled\n",
    "        if random_shuffle:\n",
    "            ind = np.array(range(len(self.frontal_indices)))\n",
    "            shuffle(ind)\n",
    "            self.profiles = self.profiles[ind]\n",
    "            self.frontal_indices = self.frontal_indices[ind]\n",
    "            print(len(self.frontal_indices) )\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        self.n = len(self.frontal_indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        profiles = []\n",
    "        frontals = []\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            profile_filename = self.profiles[self.i]\n",
    "            frontal_filename = self.frontals[self.frontal_indices[self.i]]\n",
    "\n",
    "            # Use PIL to decode JPEG images\n",
    "            with Image.open(profile_filename) as profile_img:\n",
    "                profiles.append(np.array(profile_img))\n",
    "            with Image.open(frontal_filename) as frontal_img:\n",
    "                frontals.append(np.array(frontal_img))\n",
    "\n",
    "            self.i = (self.i + 1) % self.n\n",
    "        return (profiles, frontals)\n",
    "\n",
    "\n",
    "\"\"\"class ImagePipeline:\n",
    "    def __init__(self, imageset_dir, image_size=128, random_shuffle=False, batch_size=64, num_threads=2, device_id=0):\n",
    "        self.eii = ExternalInputIterator(imageset_dir, batch_size, random_shuffle)\n",
    "        self.iterator = iter(self.eii)\n",
    "        self.num_inputs = len(self.eii.frontal_indices)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def epoch_size(self, name=None):\n",
    "        return self.num_inputs\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return self.num_inputs\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        (images, targets) = next(self.iterator)\n",
    "\n",
    "        # Perform resizing and normalization using NumPy\n",
    "        resized_images = np.array([np.array(Image.fromarray(img).resize((self.image_size, self.image_size))) for img in images])\n",
    "        resized_targets = np.array([np.array(Image.fromarray(target).resize((self.image_size, self.image_size))) for target in targets])\n",
    "\n",
    "        # Normalize using mean and standard deviation\n",
    "        normalized_images = (resized_images - 128.0) / 128.0\n",
    "        normalized_targets = (resized_targets - 128.0) / 128.0\n",
    "\n",
    "        return (normalized_images, normalized_targets)\"\"\"\n",
    "\n",
    "class ImagePipeline:\n",
    "    def __init__(self, imageset_dir, image_size=128, random_shuffle=False, batch_size=64, num_threads=2, device_id=0):\n",
    "        self.eii = ExternalInputIterator(imageset_dir, batch_size, random_shuffle)\n",
    "        self.iterator = iter(self.eii)\n",
    "        self.num_inputs = len(self.eii.frontal_indices)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def epoch_size(self, name=None):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        (images, targets) = next(self.iterator)\n",
    "\n",
    "        # Perform resizing and normalization using NumPy\n",
    "        resized_images = np.array([np.array(Image.fromarray(img).resize((self.image_size, self.image_size))) for img in images])\n",
    "        resized_targets = np.array([np.array(Image.fromarray(target).resize((self.image_size, self.image_size))) for target in targets])\n",
    "\n",
    "        # Normalize using mean and standard deviation\n",
    "        normalized_images = (resized_images - 128.0) / 128.0\n",
    "        normalized_targets = (resized_targets - 128.0) / 128.0\n",
    "\n",
    "        return (normalized_images, normalized_targets)\n",
    "\n",
    "    def __getitem__(self, index):  # Added __getitem__ method\n",
    "        # Advance the iterator to the desired index\n",
    "        for _ in range(index):\n",
    "            next(self.iterator)\n",
    "\n",
    "        # Return the next batch\n",
    "        return next(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c968013a-d49d-483d-b692-5bcacf531e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = ExternalInputIterator(imageset_dir=r\"C:\\Users\\zed\\Dataset\\cropped\",batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7888f3-6b08-42b7-bb75-6ab879a07b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from random import shuffle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image  # Import PIL for image decoding\n",
    "\n",
    "\n",
    "def is_jpeg(filename):\n",
    "    # You can customize this function to match your preferred file extension(s)\n",
    "    return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\"])\n",
    "\n",
    "\n",
    "\"\"\"def get_subdirs(directory):\n",
    "    # Ensure both \"frontal_file\" and \"profile_file\" folders exist\n",
    "    subdirs = [\n",
    "        d\n",
    "        for d in sorted(os.listdir(directory))\n",
    "        if os.path.isdir(os.path.join(directory, d))\n",
    "        and all(\n",
    "            os.path.isdir(os.path.join(d, subfolder))\n",
    "            for subfolder in [\"frontal_file\", \"profile_file\"]\n",
    "        )\n",
    "    ]\n",
    "    return sorted(subdirs)\"\"\"\n",
    "def get_subdirs(directory):\n",
    "    subdirs = sorted([join(directory, name) for name in sorted(os.listdir(directory)) if os.path.isdir(os.path.join(directory, name))])\n",
    "    return subdirs\n",
    "\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "class ExternalInputIterator:\n",
    "    def __init__(self, imageset_dir, batch_size, random_shuffle=False):\n",
    "        self.imageset_dir = imageset_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Get subdirectories (assuming \"frontal_file\" and \"profile_file\" folders exist)\n",
    "        self.person_dirs = get_subdirs(imageset_dir)\n",
    "\n",
    "        # Collect frontal image paths (adjust according to your data structure)\n",
    "        self.frontals = [\n",
    "            join(person_dir, frontal_file)\n",
    "            for person_dir in self.person_dirs\n",
    "            for frontal_file in os.listdir(person_dir)\n",
    "            if is_jpeg(frontal_file)\n",
    "        ]\n",
    "        #print(self.frontals)\n",
    "\n",
    "        # Collect profile image paths (adjust according to your data structure)\n",
    "        self.profile_files = [\n",
    "            [\n",
    "                join(person_dir, profile_file)\n",
    "                for profile_file in os.listdir(person_dir)\n",
    "                if is_jpeg(profile_file)\n",
    "            ]\n",
    "            for person_dir in self.person_dirs\n",
    "        ]\n",
    "        #print(len(self.profile_files))\n",
    "\n",
    "        # Create a flattened list of frontal indices\n",
    "        self.frontal_indices = np.array(\n",
    "            [i for i, profiles in enumerate(self.profile_files) for _ in profiles]\n",
    "        )\n",
    "        #print(len(self.frontal_indices ))\n",
    "\n",
    "        # Flatten profile_files for easier access\n",
    "        self.profiles = np.array(flatten(self.profile_files))\n",
    "        #rint(len(self.profiles ))\n",
    "\n",
    "        # Shuffle if necessary\n",
    "        if random_shuffle:\n",
    "            \"\"\"ind = np.array(range(len(self.frontal_indices)))\n",
    "            shuffle(ind)\n",
    "            self.profiles = self.profiles[ind]\n",
    "            self.frontal_indices = self.frontal_indices[ind]\"\"\"\n",
    "            ind = np.array(range(len(self.frontal_indices)))\n",
    "            shuffle(ind)\n",
    "            self.profiles = self.profiles[ind.astype(int)]\n",
    "            self.frontal_indices = self.frontal_indices[ind.astype(int)]\n",
    "\n",
    "\n",
    "        self.i = 0\n",
    "        self.n = len(self.frontal_indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        profiles = []\n",
    "        frontals = []\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            profile_filename = self.profiles[self.i]\n",
    "            frontal_filename = self.frontals[self.frontal_indices[self.i]]\n",
    "\n",
    "            with Image.open(profile_filename) as profile_img:\n",
    "                profiles.append(np.array(profile_img))\n",
    "            with Image.open(frontal_filename) as frontal_img:\n",
    "                frontals.append(np.array(frontal_img))\n",
    "\n",
    "            self.i = (self.i + 1) % self.n\n",
    "\n",
    "        return (profiles, frontals)\n",
    "\n",
    "class ImagePipeline:\n",
    "    def __init__(self, imageset_dir, image_size=128, random_shuffle=False, batch_size=64, num_threads=2, device_id=0):\n",
    "        self.eii = ExternalInputIterator(imageset_dir, batch_size, random_shuffle)\n",
    "        self.iterator = iter(self.eii)\n",
    "        self.num_inputs = len(self.eii.frontal_indices)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def epoch_size(self, name=None):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        (images, targets) = next(self.iterator)\n",
    "\n",
    "        # Perform resizing and normalization using NumPy\n",
    "        resized_images = np.array([np.array(Image.fromarray(img).resize((self.image_size, self.image_size))) for img in images])\n",
    "        resized_targets = np.array([np.array(Image.fromarray(target).resize((self.image_size, self.image_size))) for target in targets])\n",
    "\n",
    "        # Normalize using mean and standard deviation\n",
    "        normalized_images = (resized_images - 128.0) / 128.0\n",
    "        normalized_targets = (resized_targets - 128.0) / 128.0\n",
    "\n",
    "        return (normalized_images, normalized_targets)\n",
    "\n",
    "    def __getitem__(self, index):  # Added __getitem__ method\n",
    "        # Advance the iterator to the desired index\n",
    "        for _ in range(index):\n",
    "            next(self.iterator)\n",
    "\n",
    "        # Return the next batch\n",
    "        return next(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa08d9-2c68-4794-a5b8-a37e5aaec917",
   "metadata": {},
   "source": [
    "Edited data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205c4846-23ac-4c7f-aa86-5952a5753cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from random import shuffle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image  # Import PIL for image decoding\n",
    "\n",
    "\n",
    "def is_jpeg(filename):\n",
    "    # You can customize this function to match your preferred file extension(s)\n",
    "    return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\"])\n",
    "\n",
    "\n",
    "\"\"\"def get_subdirs(directory):\n",
    "    # Ensure both \"frontal_file\" and \"profile_file\" folders exist\n",
    "    subdirs = [\n",
    "        d\n",
    "        for d in sorted(os.listdir(directory))\n",
    "        if os.path.isdir(os.path.join(directory, d))\n",
    "        and all(\n",
    "            os.path.isdir(os.path.join(d, subfolder))\n",
    "            for subfolder in [\"frontal_file\", \"profile_file\"]\n",
    "        )\n",
    "    ]\n",
    "    return sorted(subdirs)\"\"\"\n",
    "def get_subdirs(directory):\n",
    "    subdirs = sorted([join(directory, name) for name in sorted(os.listdir(directory)) if os.path.isdir(os.path.join(directory, name))])\n",
    "    return subdirs\n",
    "\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "class ExternalInputIterator:\n",
    "    def __init__(self, imageset_dir, batch_size, random_shuffle=False):\n",
    "        self.imageset_dir = imageset_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Get subdirectories (assuming \"frontal_file\" and \"profile_file\" folders exist)\n",
    "        self.person_dirs = get_subdirs(imageset_dir)\n",
    "\n",
    "        # Collect frontal image paths (adjust according to your data structure)\n",
    "        self.frontals = np.array(\n",
    "            [join(imageset_dir, frontal_file) for frontal_file in sorted(os.listdir(imageset_dir)) if is_jpeg(frontal_file)]\n",
    "        )\n",
    "        print(len(self.frontals))\n",
    "\n",
    "        # Collect profile image paths (adjust according to your data structure)\n",
    "        self.profile_files = [\n",
    "            [\n",
    "                join(person_dir, profile_file)\n",
    "                for profile_file in os.listdir(person_dir)\n",
    "                if is_jpeg(profile_file)\n",
    "            ]\n",
    "            for person_dir in self.person_dirs\n",
    "        ]\n",
    "        #print(len(self.profile_files))\n",
    "\n",
    "        # Create a flattened list of frontal indices\n",
    "        self.frontal_indices = np.array(\n",
    "            [i for i, profiles in enumerate(self.profile_files) for _ in profiles]\n",
    "        )\n",
    "        print(len(self.frontal_indices ))\n",
    "\n",
    "        # Flatten profile_files for easier access\n",
    "        self.profiles = np.array(flatten(self.profile_files))\n",
    "        print(len(self.profiles ))\n",
    "\n",
    "        # Shuffle if necessary\n",
    "        if random_shuffle:\n",
    "            \"\"\"ind = np.array(range(len(self.frontal_indices)))\n",
    "            shuffle(ind)\n",
    "            self.profiles = self.profiles[ind]\n",
    "            self.frontal_indices = self.frontal_indices[ind]\"\"\"\n",
    "            ind = np.array(range(len(self.frontal_indices)))\n",
    "            shuffle(ind)\n",
    "            self.profiles = self.profiles[ind.astype(int)]\n",
    "            self.frontal_indices = self.frontal_indices[ind.astype(int)]\n",
    "\n",
    "\n",
    "        self.i = 0\n",
    "        self.n = len(self.frontal_indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        profiles = []\n",
    "        frontals = []\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            profile_filename = self.profiles[self.i]\n",
    "            #frontal_filename = self.frontals[self.frontal_indices[self.i]]\n",
    "            frontal_filename = self.frontals[self.i]\n",
    "\n",
    "            with Image.open(profile_filename) as profile_img:\n",
    "                profiles.append(np.array(profile_img))\n",
    "            with Image.open(frontal_filename) as frontal_img:\n",
    "                frontals.append(np.array(frontal_img))\n",
    "\n",
    "            self.i = (self.i + 1) % self.n\n",
    "\n",
    "        return (profiles, frontals)\n",
    "\n",
    "class ImagePipeline:\n",
    "    def __init__(self, imageset_dir, image_size=128, random_shuffle=False, batch_size=64, num_threads=2, device_id=0):\n",
    "        self.eii = ExternalInputIterator(imageset_dir, batch_size, random_shuffle)\n",
    "        self.iterator = iter(self.eii)\n",
    "        self.num_inputs = len(self.eii.frontal_indices)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def epoch_size(self, name=None):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        (images, targets) = next(self.iterator)\n",
    "\n",
    "        # Perform resizing and normalization using NumPy\n",
    "        resized_images = np.array([np.array(Image.fromarray(img).resize((self.image_size, self.image_size))) for img in images])\n",
    "        resized_targets = np.array([np.array(Image.fromarray(target).resize((self.image_size, self.image_size))) for target in targets])\n",
    "\n",
    "        # Normalize using mean and standard deviation\n",
    "        normalized_images = (resized_images - 128.0) / 128.0\n",
    "        normalized_targets = (resized_targets - 128.0) / 128.0\n",
    "\n",
    "        return (normalized_images, normalized_targets)\n",
    "\n",
    "    def __getitem__(self, index):  # Added __getitem__ method\n",
    "        # Advance the iterator to the desired index\n",
    "        for _ in range(index):\n",
    "            next(self.iterator)\n",
    "\n",
    "        # Return the next batch\n",
    "        return next(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9393dd9-6dc7-44b0-982b-b3c40199f17c",
   "metadata": {},
   "source": [
    "network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1d2639-39e7-4dbf-9c08-83dd1142a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "''' Generator network for 128x128 RGB images '''\n",
    "class G(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # Input HxW = 128x128\n",
    "            nn.Conv2d(3, 16, 4, 2, 1), # Output HxW = 64x64\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 4, 2, 1), # Output HxW = 32x32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), # Output HxW = 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # Output HxW = 8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), # Output HxW = 4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1), # Output HxW = 2x2\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            # At this point, we arrive at our low D representation vector, which is 512 dimensional.\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 1, 0, bias = False), # Output HxW = 4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # Output HxW = 8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), # Output HxW = 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias = False), # Output HxW = 32x32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias = False), # Output HxW = 64x64\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 3, 4, 2, 1, bias = False), # Output HxW = 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "''' Discriminator network for 128x128 RGB images '''\n",
    "class D(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "                                  nn.Conv2d(3, 16, 4, 2, 1),\n",
    "                                  nn.LeakyReLU(0.2, inplace = True),\n",
    "                                  nn.Conv2d(16, 32, 4, 2, 1),\n",
    "                                  nn.BatchNorm2d(32),\n",
    "                                  nn.LeakyReLU(0.2, inplace = True),\n",
    "                                  nn.Conv2d(32, 64, 4, 2, 1),\n",
    "                                  nn.BatchNorm2d(64),\n",
    "                                  nn.LeakyReLU(0.2, inplace = True),\n",
    "                                  nn.Conv2d(64, 128, 4, 2, 1),\n",
    "                                  nn.BatchNorm2d(128),\n",
    "                                  nn.LeakyReLU(0.2, inplace = True),\n",
    "                                  nn.Conv2d(128, 256, 4, 2, 1),\n",
    "                                  nn.BatchNorm2d(256),\n",
    "                                  nn.LeakyReLU(0.2, inplace = True),\n",
    "                                  nn.Conv2d(256, 512, 4, 2, 1),\n",
    "                                  nn.BatchNorm2d(512),\n",
    "                                  nn.LeakyReLU(0.2, inplace = True),\n",
    "                                  nn.Conv2d(512, 1, 4, 2, 1, bias = False),\n",
    "                                  nn.Sigmoid()\n",
    "                                  )\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ffd32-04a6-4ca7-9b48-e7f321a9f56c",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32bd8b7a-23ac-44b4-98e1-c9e663af4c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "111\n",
      "111\n",
      "starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████| 111/111 [05:17<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training epoch completed in  317.498330116272  seconds\n",
      "[1/30] Training absolute losses: L1 0.3119904 ; L2 0.1406782 BCE 7.2579693\n",
      "starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████████| 111/111 [05:20<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 is starting..\n",
      "[2/30] Training absolute losses: L1 0.2082021 ; L2 0.0687924 BCE 5.7170552\n",
      "starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████| 111/111 [05:16<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 is starting..\n",
      "[3/30] Training absolute losses: L1 0.1621829 ; L2 0.0447100 BCE 4.1597956\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "#from data import ImagePipeline\n",
    "#import network\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(10)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(999)\n",
    "torch.cuda.manual_seed(999)\n",
    "# Where is your training dataset at?\n",
    "datapath = r\"C:\\Users\\zed\\Dataset\\cropped\\frontal_file\"\n",
    "\n",
    "# You can also choose which GPU you want your model to be trained on below:\n",
    "gpu_id = 0\n",
    "device = torch.device(\"cuda\", gpu_id)\n",
    "\n",
    "\"\"\"train_pipe = ImagePipeline(datapath, image_size=128, random_shuffle=True, batch_size=30, device_id=gpu_id)\n",
    "train_pipe.build()\n",
    "m_train = train_pipe.epoch_size()\n",
    "print(\"Size of the training set: \", m_train)\n",
    "train_pipe_loader = DALIGenericIterator(train_pipe, [\"profiles\", \"frontals\"], m_train)\"\"\"\n",
    "# Assuming you have the modified ImagePipeline class from the previous responses\n",
    "train_pipe = ImagePipeline(datapath, image_size=128, random_shuffle=True, batch_size=32, device_id=gpu_id)\n",
    "# No need to call build() without DALI\n",
    "\n",
    "# Use a standard PyTorch DataLoader instead of DALIGenericIterator\n",
    "#train_pipe_loader = DataLoader(train_pipe, batch_size=train_pipe.batch_size)\n",
    "m_train = train_pipe.epoch_size()\n",
    "#train_pipe_loader = DataLoader(train_pipe, batch_size=32,)\n",
    "train_pipe_loader = DataLoader(train_pipe,)\n",
    "# Generator:\n",
    "#netG = network.G().to(device)\n",
    "#netG.apply(network.weights_init)\n",
    "netG = G().to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Discriminator:\n",
    "#netD = network.D().to(device)\n",
    "#netD.apply(network.weights_init)\n",
    "netD = D().to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Here is where you set how important each component of the loss function is:\n",
    "L1_factor = 0\n",
    "L2_factor = 1\n",
    "GAN_factor = 0.0005\n",
    "\n",
    "criterion = nn.BCELoss() # Binary cross entropy loss\n",
    "\n",
    "# Optimizers for the generator and the discriminator (Adam is a fancier version of gradient descent with a few more bells and whistles that is used very often):\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999), eps = 1e-8)\n",
    "\n",
    "# Create a directory for the output files\n",
    "try:\n",
    "    os.mkdir('output')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's train for 30 epochs (meaning, we go through the entire training set 30 times):\n",
    "for epoch in range(3):\n",
    "    \n",
    "    # Lets keep track of the loss values for each epoch:\n",
    "    loss_L1 = 0\n",
    "    loss_L2 = 0\n",
    "    loss_gan = 0\n",
    "    \n",
    "    # Your train_pipe_loader will load the images one batch at a time\n",
    "    # The inner loop iterates over those batches:\n",
    "    print(\"starting...\")\n",
    "    with tqdm(total=len(train_pipe_loader), desc=f\"Epoch {epoch+1}\") as pbar:\n",
    "        for i, data in enumerate(train_pipe_loader, 0):\n",
    "        \n",
    "        # These are your images from the current batch:\n",
    "        #profile = data[0]['profiles']\n",
    "        #frontal = data[0]['frontals']\n",
    "            profile = data[0].view(32, 3, 128, 128)\n",
    "            frontal = data[1].view(32, 3, 128, 128)\n",
    "        \n",
    "        \n",
    "        # TRAINING THE DISCRIMINATOR\n",
    "            netD.zero_grad()\n",
    "            real = Variable(frontal).type('torch.FloatTensor').to(device)\n",
    "            target = Variable(torch.ones(real.size()[0])).to(device)\n",
    "            output = netD(real)\n",
    "        # D should accept the GT images\n",
    "            errD_real = criterion(output, target)\n",
    "        \n",
    "            profile = Variable(profile).type('torch.FloatTensor').to(device)\n",
    "            generated = netG(profile)\n",
    "            target = Variable(torch.zeros(real.size()[0])).to(device)\n",
    "            output = netD(generated.detach()) # detach() because we are not training G here\n",
    "\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # D should reject the synthetic images\n",
    "            errD_fake = criterion(output, target)\n",
    "        \n",
    "            errD = errD_real + errD_fake\n",
    "            errD.backward()\n",
    "        # Update D\n",
    "            optimizerD.step()\n",
    "        \n",
    "        # TRAINING THE GENERATOR\n",
    "            netG.zero_grad()\n",
    "            target = Variable(torch.ones(real.size()[0])).to(device)\n",
    "            output = netD(generated)\n",
    "        \n",
    "        # G wants to :\n",
    "        # (a) have the synthetic images be accepted by D (= look like frontal images of people)\n",
    "            errG_GAN = criterion(output, target)\n",
    "        \n",
    "        # (b) have the synthetic images resemble the ground truth frontal image\n",
    "            errG_L1 = torch.mean(torch.abs(real - generated))\n",
    "            errG_L2 = torch.mean(torch.pow((real - generated), 2))\n",
    "        \n",
    "            errG = GAN_factor * errG_GAN + L1_factor * errG_L1 + L2_factor * errG_L2\n",
    "        \n",
    "            loss_L1 += errG_L1.item()\n",
    "            loss_L2 += errG_L2.item()\n",
    "            loss_gan += errG_GAN.item()\n",
    "        \n",
    "            errG.backward()\n",
    "        # Update G\n",
    "            optimizerG.step()\n",
    "    \n",
    "    if epoch == 0:\n",
    "        print('First training epoch completed in ',(time.time() - start_time),' seconds')\n",
    "    if epoch > 0:\n",
    "        print(f\"Epoch: {epoch} is starting..\")\n",
    "    # reset the DALI iterator\n",
    "    #train_pipe_loader.reset()\n",
    "\n",
    "    # Print the absolute values of three losses to screen:\n",
    "    print('[%d/30] Training absolute losses: L1 %.7f ; L2 %.7f BCE %.7f' % ((epoch + 1), loss_L1/m_train, loss_L2/m_train, loss_gan/m_train,))\n",
    "\n",
    "    # Save the inputs, outputs, and ground truth frontals to files:\n",
    "    vutils.save_image(profile.data, 'output/%03d_input.jpg' % epoch, normalize=True)\n",
    "    vutils.save_image(real.data, 'output/%03d_real.jpg' % epoch, normalize=True)\n",
    "    vutils.save_image(generated.data, 'output/%03d_generated.jpg' % epoch, normalize=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # Save the pre-trained Generator as well\n",
    "    torch.save(netG,'output/netG_%d.pt' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e1ca5e8-191e-4cfe-8071-d6d3a3810956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set:  211\n",
      "211\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "#from data import ImagePipeline\n",
    "#import network\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(10)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(999)\n",
    "\n",
    "# Where is your training dataset at?\n",
    "datapath = r\"C:\\Users\\zed\\Dataset\\cropped\"\n",
    "\n",
    "# You can also choose which GPU you want your model to be trained on below:\n",
    "gpu_id = 0\n",
    "device = torch.device(\"cuda\", gpu_id)\n",
    "\n",
    "\"\"\"train_pipe = ImagePipeline(datapath, image_size=128, random_shuffle=True, batch_size=30, device_id=gpu_id)\n",
    "train_pipe.build()\n",
    "m_train = train_pipe.epoch_size()\n",
    "print(\"Size of the training set: \", m_train)\n",
    "train_pipe_loader = DALIGenericIterator(train_pipe, [\"profiles\", \"frontals\"], m_train)\"\"\"\n",
    "# Assuming you have the modified ImagePipeline class from the previous responses\n",
    "train_pipe = ImagePipeline(datapath, image_size=128, random_shuffle=True, batch_size=32,device_id=gpu_id)\n",
    "# No need to call build() without DALI\n",
    "\n",
    "# Use a standard PyTorch DataLoader instead of DALIGenericIterator\n",
    "#train_pipe_loader = DataLoader(train_pipe, batch_size=train_pipe.batch_size)\n",
    "\n",
    "train_pipe_loader = DataLoader(train_pipe)\n",
    "\n",
    "m_train = train_pipe.epoch_size()\n",
    "print(\"Size of the training set: \", m_train)\n",
    "print(len(train_pipe_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ab9ec7-7b1b-4fda-9d35-21b4221996a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': <__main__.ImagePipeline object at 0x0000022A09AD0E90>, 'num_workers': 0, 'prefetch_factor': None, 'pin_memory': False, 'pin_memory_device': '', 'timeout': 0, 'worker_init_fn': None, '_DataLoader__multiprocessing_context': None, '_dataset_kind': 0, 'batch_size': 30, 'drop_last': False, 'sampler': <torch.utils.data.sampler.SequentialSampler object at 0x0000022A09AD0750>, 'batch_sampler': <torch.utils.data.sampler.BatchSampler object at 0x0000022A09AD12D0>, 'generator': None, 'collate_fn': <function default_collate at 0x0000022A06A75080>, 'persistent_workers': False, '_DataLoader__initialized': True, '_IterableDataset_len_called': None, '_iterator': None}\n"
     ]
    }
   ],
   "source": [
    "print(train_pipe_loader.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "43ed6af1-4576-4dc5-8cdd-6fedc647619b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "First element of data_list is not a dictionary. Check data structure.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Check if data_list contains dictionaries\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_list[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst element of data_list is not a dictionary. Check data structure.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Extract and print keys\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_list:\n",
      "\u001b[1;31mTypeError\u001b[0m: First element of data_list is not a dictionary. Check data structure."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have the modified ImagePipeline class and train_pipe_loader initialization...\n",
    "\n",
    "# Check if train_pipe_loader is iterable\n",
    "if not hasattr(train_pipe_loader, '__iter__'):\n",
    "    raise TypeError(\"train_pipe_loader is not iterable. Double-check its setup.\")\n",
    "\n",
    "# Convert to a list for processing (optional, but safer for data handling)\n",
    "data_list = list(train_pipe_loader)\n",
    "\n",
    "# Check if data_list contains dictionaries\n",
    "if not isinstance(data_list[0], dict):\n",
    "    raise TypeError(\"First element of data_list is not a dictionary. Check data structure.\")\n",
    "\n",
    "# Extract and print keys\n",
    "for data in data_list:\n",
    "    print(\"Keys of current dictionary:\", list(data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33dfd95-487e-4802-a217-1dfadab0b63f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in C:\\Users\\zed\\Dataset\\cropped\\profile_file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m      7\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m128\u001b[39m),  \u001b[38;5;66;03m# Resize to image_size\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),   \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))  \u001b[38;5;66;03m# Optional normalization\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load datasets using PyTorch's ImageFolder\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m profiles_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mzed\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDataset\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcropped\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mprofile_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m frontals_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mzed\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcropped\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfrontal_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(profiles_dataset))\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\torchvision\\datasets\\folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\torchvision\\datasets\\folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\torchvision\\datasets\\folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     40\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in C:\\Users\\zed\\Dataset\\cropped\\profile_file."
     ]
    }
   ],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define image transformations (adjust as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(128),  # Resize to image_size\n",
    "    transforms.ToTensor(),   # Convert to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Optional normalization\n",
    "])\n",
    "\n",
    "# Load datasets using PyTorch's ImageFolder\n",
    "profiles_dataset = datasets.ImageFolder(r\"C:\\Users\\zed\\Dataset\\cropped\\profile_file\", transform=transform)\n",
    "frontals_dataset = datasets.ImageFolder(r\"C:\\Users\\zed\\Dataset\\cropped\\frontal_file\", transform=transform)\n",
    "print(len(profiles_dataset))\n",
    "print(len(frontals_dataset))\n",
    "# Create dataloaders\n",
    "profiles_loader = torch.utils.data.DataLoader(profiles_dataset, batch_size=batch_size, shuffle=True)\n",
    "frontals_loader = torch.utils.data.DataLoader(frontals_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Function for displaying images without DALI\n",
    "def show_images(image_batch):\n",
    "    columns = 4\n",
    "    rows = (image_batch.shape[0] + 1) // columns\n",
    "    fig = plt.figure(figsize=(32, (32 // columns) * rows))\n",
    "    gs = gridspec.GridSpec(rows, columns)\n",
    "\n",
    "    for j in range(image_batch.shape[0]):\n",
    "        plt.subplot(gs[j])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image_batch[j].permute(1, 2, 0))  # Permute for correct display\n",
    "\n",
    "# Get image batches\n",
    "profiles_batch = next(iter(profiles_loader))[0]\n",
    "frontals_batch = next(iter(frontals_loader))[0]\n",
    "\n",
    "# Display images\n",
    "show_images(profiles_batch)\n",
    "show_images(frontals_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339fb272-f86e-4a8a-b1b1-caef1caa25a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_pipe_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 142\u001b[0m, in \u001b[0;36mImagePipeline.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):  \u001b[38;5;66;03m# Added __getitem__ method\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Advance the iterator to the desired index\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(index):\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# Return the next batch\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 104\u001b[0m, in \u001b[0;36mExternalInputIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m         profiles\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(profile_img))\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(frontal_filename) \u001b[38;5;28;01mas\u001b[39;00m frontal_img:\n\u001b[1;32m--> 104\u001b[0m         frontals\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(frontal_img))\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (profiles, frontals)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\PIL\\Image.py:673\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 673\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\PIL\\Image.py:732\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m args \u001b[38;5;241m==\u001b[39m ():\n\u001b[0;32m    730\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m--> 732\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tor\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_pipe_loader, 0):\n",
    "    x = data[0]\n",
    "    if len(x) > 1:\n",
    "        break\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5edeef0-c182-4f3d-9f29-3157f06e7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have access to 'train_pipe_loader'\n",
    "\n",
    "# Choose an index to access a specific image (replace with your desired index)\n",
    "image_index = 0\n",
    "\n",
    "# Extract the image (adapt based on your data structure)\n",
    "image = None\n",
    "for i, data in enumerate(train_pipe_loader, 0):\n",
    "    if i == image_index:\n",
    "        image = data  # Assuming data contains the image\n",
    "        break\n",
    "\n",
    "# Convert to NumPy array if needed\n",
    "if isinstance(image, torch.Tensor):\n",
    "    image = image.cpu().numpy()\n",
    "\n",
    "# Process the image for display (optional)\n",
    "# ... (e.g., normalize, convert channels)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
