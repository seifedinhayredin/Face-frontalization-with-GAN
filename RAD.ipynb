{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d6c2b-c400-4f95-a4d3-54b9e2de6da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07719e58-b3af-4d07-bfcc-1453cbd2261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def is_jpeg(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\"])\n",
    "\n",
    "def get_subdirs(directory):\n",
    "    subdirs = sorted([os.path.join(directory, name) for name in sorted(os.listdir(directory)) if os.path.isdir(os.path.join(directory, name))])\n",
    "    return subdirs\n",
    "\n",
    "class ExternalInputIterator:\n",
    "    def __init__(self, imageset_dir, batch_size, random_shuffle=False):\n",
    "        self.imageset_dir = imageset_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Get subdirectories (assuming \"pose\" and \"frontal\" folders exist)\n",
    "        #self.pose_dirs = get_subdirs(os.path.join(imageset_dir, \"pose\"))\n",
    "        self.pose_dirs = os.path.join(imageset_dir, \"pose\")\n",
    "        self.frontal_dir = os.path.join(imageset_dir, \"frontal\")\n",
    "        print(self.frontal_dir)\n",
    "        print(self.pose_dirs)\n",
    "\n",
    "        # Collect profile image paths\n",
    "        self.profile_files = []\n",
    "        #for pose_dir in self.pose_dirs:\n",
    "        profile_files = [os.path.join(self.pose_dirs, file) for file in sorted(os.listdir(self.pose_dirs)) if is_jpeg(file)]\n",
    "        self.profile_files.extend(profile_files)\n",
    "        print(len(self.profile_files))\n",
    "\n",
    "        # Collect frontal image paths\n",
    "        self.frontal_files = [os.path.join(self.frontal_dir, file) for file in sorted(os.listdir(self.frontal_dir)) if is_jpeg(file)]\n",
    "        print(len(self.frontal_files))\n",
    "\n",
    "        # Shuffle if necessary\n",
    "        if random_shuffle:\n",
    "            np.random.shuffle(self.profile_files)\n",
    "            np.random.shuffle(self.frontal_files)\n",
    "\n",
    "        self.i = 0\n",
    "        self.n = len(self.profile_files)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        profiles = []\n",
    "        frontals = []\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            profile_filename = self.profile_files[self.i]\n",
    "            frontal_filename = self.match_frontal_image(profile_filename)\n",
    "\n",
    "            with Image.open(profile_filename) as profile_img:\n",
    "                profiles.append(np.array(profile_img))\n",
    "            with Image.open(frontal_filename) as frontal_img:\n",
    "                frontals.append(np.array(frontal_img))\n",
    "\n",
    "            self.i = (self.i + 1) % self.n\n",
    "\n",
    "        return (profiles, frontals)\n",
    "\n",
    "    def match_frontal_image(self, profile_filename):\n",
    "        profile_name = os.path.basename(profile_filename).split(\"_\")[0]\n",
    "        for frontal_file in self.frontal_files:\n",
    "            if profile_name in frontal_file:\n",
    "                return frontal_file\n",
    "        return None\n",
    "\n",
    "class ImagePipeline:\n",
    "    def __init__(self, imageset_dir, image_size=128, random_shuffle=False, batch_size=64,device_id = 0):\n",
    "        self.eii = ExternalInputIterator(imageset_dir, batch_size, random_shuffle)\n",
    "        self.iterator = iter(self.eii)\n",
    "        self.num_inputs = len(self.eii.profile_files)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def epoch_size(self, name=None):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_inputs\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        (images, targets) = next(self.iterator)\n",
    "\n",
    "        # Perform resizing and normalization using NumPy\n",
    "        resized_images = np.array([np.array(Image.fromarray(img).resize((self.image_size, self.image_size))) for img in images])\n",
    "        resized_targets = np.array([np.array(Image.fromarray(target).resize((self.image_size, self.image_size))) for target in targets])\n",
    "\n",
    "        # Calculate mean and standard deviation for each channel separately\n",
    "        #mean = np.array([0.5, 0.5, 0.5])  # Assuming RGB images have pixel values in [0, 255] range\n",
    "        #std = np.array([0.5, 0.5, 0.5])   # Assuming RGB images have pixel values in [0, 255] range\n",
    "        \n",
    "        # Normalize each channel independently\n",
    "        #normalized_images = (resized_images / 255.0 - mean) / std\n",
    "        #normalized_targets = (resized_targets / 255.0 - mean) / std\n",
    "\n",
    "\n",
    "        # Normalize using mean and standard deviation\n",
    "        normalized_images = (resized_images - 128.0) / 128.0\n",
    "        normalized_targets = (resized_targets - 128.0) / 128.0\n",
    "\n",
    "        return (normalized_images, normalized_targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Advance the iterator to the desired index\n",
    "        for _ in range(index):\n",
    "            next(self.iterator)\n",
    "\n",
    "        # Return the next batch\n",
    "        return next(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5053a-3e1e-4ec1-83ed-92450d0e9af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e795101-6398-4aa4-b31c-1b1941797084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16132b7-a359-4ae4-bee3-6ff880f0c575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37cb61a-0812-4959-919e-abf9cf7eaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' Generator network for 128x128 RGB images '''\n",
    "class G(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # Input HxW = 128x128\n",
    "            nn.Conv2d(1, 16, 4, 2, 1), # Output HxW = 64x64\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 4, 2, 1), # Output HxW = 32x32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), # Output HxW = 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # Output HxW = 8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), # Output HxW = 4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1), # Output HxW = 2x2\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            # At this point, we arrive at our low D representation vector, which is 512 dimensional.\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 1, 0, bias = False), # Output HxW = 4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # Output HxW = 8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), # Output HxW = 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias = False), # Output HxW = 32x32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias = False), # Output HxW = 64x64\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, 4, 2, 1, bias = False), # Output HxW = 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090b069-c82d-4f05-bc31-08fcbe6934a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e43df854-e18e-43e5-8a72-8843db04d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RelativeAvgDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAvgDiscriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(16, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 2, 1, bias=False),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, real, fake):\n",
    "        #print(real.shape)\n",
    "        #print(fake.shape)\n",
    "        # Concatenate real and fake data along a new dimension\n",
    "        #input_concat = torch.cat((real, fake), dim=1)\n",
    "        input_concat = torch.cat((real.squeeze(0), fake.squeeze(0)), dim=0)\n",
    "        #print(input_concat.shape)\n",
    "        output = self.main(input_concat)\n",
    "        \n",
    "        return output.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe5eb5-afe0-4755-ad6a-8f88a14c73ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7199ad7-006a-4aba-b847-717397562dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e18c78-6f33-4da7-8b83-1494ff15762e",
   "metadata": {},
   "source": [
    "**Edited RAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e63dae31-c41e-43c5-bbd1-a52dd4f1ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RelativeAvgDiscriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(RelativeAvgDiscriminator, self).__init__()\n",
    "\n",
    "    # Separate feature extraction for real and generated data\n",
    "    self.conv_real = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, 4, 2, 1),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(16, 32, 4, 2, 1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(32, 64, 4, 2, 1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "    )\n",
    "    self.conv_generated = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, 4, 2, 1),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(16, 32, 4, 2, 1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(32, 64, 4, 2, 1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "    )\n",
    "\n",
    "    # Relative Average Pooling\n",
    "    self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # Remaining convolutional layers (modified for combined features)\n",
    "    self.post_pool = nn.Sequential(\n",
    "        nn.Conv2d(128, 128, 4, 2, 1),  # Input channels changed to 128\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.Conv2d(128, 256, 4, 2, 1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # ... rest of the layers (same as original code)\n",
    "    )\n",
    "\n",
    "    # Output layer with sigmoid activation\n",
    "    self.output = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, real, fake):\n",
    "    # Extract features from real and generated data\n",
    "    real_features = self.conv_real(real)\n",
    "    generated_features = self.conv_generated(fake)\n",
    "\n",
    "    # Concatenate features before pooling\n",
    "    combined_features = torch.cat([real_features, generated_features], dim=1)\n",
    "\n",
    "    # Relative Average Pooling\n",
    "    features = self.avgpool(combined_features)\n",
    "\n",
    "    # Process features with remaining layers\n",
    "    output = self.post_pool(features)\n",
    "\n",
    "    # Probability score\n",
    "    #probability = self.output(logits)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b1387-a7bd-4450-8f64-af58ca9bf35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c397e3a-02d7-4f96-838f-dcc254c208d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zed\\Dataset\\\\Grayscale_Dataset\\frontal\n",
      "C:\\Users\\zed\\Dataset\\\\Grayscale_Dataset\\pose\n",
      "200\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "#from data import ImagePipeline\n",
    "#import network\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(10)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(999)\n",
    "torch.cuda.manual_seed(999)\n",
    "# Where is your training dataset at?\n",
    "datapath =r\"C:\\Users\\zed\\Dataset\\\\Grayscale_Dataset\"\n",
    "\n",
    "# You can also choose which GPU you want your model to be trained on below:\n",
    "gpu_id = 0\n",
    "device = torch.device(\"cuda\", gpu_id)\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "\"\"\"train_pipe = ImagePipeline(datapath, image_size=128, random_shuffle=True, batch_size=30, device_id=gpu_id)\n",
    "train_pipe.build()\n",
    "m_train = train_pipe.epoch_size()\n",
    "print(\"Size of the training set: \", m_train)\n",
    "train_pipe_loader = DALIGenericIterator(train_pipe, [\"profiles\", \"frontals\"], m_train)\"\"\"\n",
    "# Assuming you have the modified ImagePipeline class from the previous responses\n",
    "train_pipe = ImagePipeline(datapath, image_size=128, random_shuffle=True, batch_size=32, device_id=gpu_id)\n",
    "# No need to call build() without DALI\n",
    "\n",
    "# Use a standard PyTorch DataLoader instead of DALIGenericIterator\n",
    "#train_pipe_loader = DataLoader(train_pipe, batch_size=train_pipe.batch_size)\n",
    "m_train = train_pipe.epoch_size()\n",
    "#train_pipe_loader = DataLoader(train_pipe, batch_size=32,)\n",
    "train_pipe_loader = DataLoader(train_pipe,)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb93941-2b8c-4067-9f0b-3736017bbb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e7b4ef2-8986-4745-a895-73d0bfae8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Define a function to calculate PSNR\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = F.mse_loss(img1, img2)\n",
    "    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "    return psnr.item()\n",
    "\n",
    "# Define a function to calculate SSIM\n",
    "# Define a function to calculate SSIM\n",
    "def calculate_ssim(img1, img2):\n",
    "    # Ensure tensors are on the same device\n",
    "    if img1.device != img2.device:\n",
    "        raise ValueError(\"Input tensors must be on the same device\")\n",
    "\n",
    "    # Calculate SSIM directly on GPU tensors\n",
    "    img1 = img1.detach().squeeze().clamp(0, 1).cpu().numpy()  # Ensure pixel values are in [0, 1] range\n",
    "    img2 = img2.detach().squeeze().clamp(0, 1).cpu().numpy()  # Ensure pixel values are in [0, 1] range\n",
    "    return ssim(img1.transpose(1, 2, 0), img2.transpose(1, 2, 0), multichannel=True, data_range=1)\n",
    "\n",
    "\n",
    "# Define lists to store PSNR and SSIM values for each epoch\n",
    "psnr_values = []\n",
    "ssim_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797f642-78d3-43b8-a857-ca9bc13ee954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a3013933-c485-4b3e-a9fa-1f92cd9bcb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████| 200/200 [09:20<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Training absolute losses: L1 0.1610392; L2 0.0519637; BCE 4.7296138\n",
      "[1/3] Average PSNR: 14.86, Average SSIM: 0.5734\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████████| 200/200 [09:22<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/3] Training absolute losses: L1 0.0788424; L2 0.0107688; BCE 6.2342537\n",
      "[2/3] Average PSNR: 19.71, Average SSIM: 0.8646\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████| 200/200 [09:21<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/3] Training absolute losses: L1 0.0737215; L2 0.0092869; BCE 7.2429958\n",
      "[3/3] Average PSNR: 20.34, Average SSIM: 0.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "netG = G().to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = RelativeAvgDiscriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "L1_factor = 0\n",
    "L2_factor = 1\n",
    "GAN_factor = 0.005\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999), eps=1e-8)\n",
    "\n",
    "try:\n",
    "    os.mkdir('output_RAD')\n",
    "except OSError:\n",
    "    pass\n",
    "    \n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Assuming 3 epochs for demonstration\n",
    "    \n",
    "    # Track loss values for each epoch\n",
    "    loss_L1 = 0\n",
    "    loss_L2 = 0\n",
    "    loss_gan = 0\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    \n",
    "    print(\"Starting epoch\", epoch + 1)\n",
    "    with tqdm(total=len(train_pipe_loader), desc=f\"Epoch {epoch+1}\") as pbar:\n",
    "        for i, data in enumerate(train_pipe_loader, 0):\n",
    "            profile = data[0].view(32, 1, 128, 128).to(device)  # Reshape and move to device\n",
    "            frontal = data[1].view(32, 1, 128, 128).to(device)  # Reshape and move to device\n",
    "\n",
    "            # TRAINING THE DISCRIMINATOR\n",
    "            netD.zero_grad()\n",
    "            optimizerD.zero_grad()\n",
    "\n",
    "            real = Variable(frontal).type('torch.FloatTensor').to(device)\n",
    "            target = Variable(torch.ones(real.size()[0])).to(device)\n",
    "            profile = Variable(profile).type('torch.FloatTensor').to(device)\n",
    "            \n",
    "            real_output = netD(real,real)  # Discriminator output for real images\n",
    "            generated = netG(profile)  # Generate images from profile\n",
    "            fake_output = netD(profile, generated.detach())  # Discriminator output for fake images\n",
    "\n",
    "            # Concatenate real and fake outputs along a new dimension\n",
    "            concatenated = torch.cat((real_output, fake_output), dim=0)\n",
    "\n",
    "            # Create labels for real and fake images\n",
    "            target_real = torch.ones_like(real_output)\n",
    "            target_fake = torch.zeros_like(fake_output)\n",
    "            targets = torch.cat((target_real, target_fake), dim=0)\n",
    "\n",
    "            # Calculate BCE loss for the concatenated outputs\n",
    "            #errD = F.binary_cross_entropy_with_logits(concatenated, targets)\n",
    "\n",
    "            errD = criterion(concatenated, targets.float())\n",
    "            errD.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # TRAINING THE GENERATOR\n",
    "            netG.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "            generated = netG(profile)\n",
    "            output = netD(profile, generated)\n",
    "\n",
    "            # G wants to have the synthetic images be accepted by D\n",
    "            errG_GAN = criterion(output, torch.ones_like(output).float())\n",
    "\n",
    "            # Calculate L1 and L2 loss between generated and real images\n",
    "            errG_L1 = F.l1_loss(generated, frontal.float())\n",
    "            errG_L2 = F.mse_loss(generated, frontal.float())\n",
    "           \n",
    "            #errG_L1 = torch.mean(torch.abs(real - generated))\n",
    "            #errG_L2 = torch.mean(torch.pow((real - generated), 2))\n",
    "            \n",
    "            # Total generator loss\n",
    "            errG = GAN_factor * errG_GAN + L1_factor * errG_L1 + L2_factor * errG_L2\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Update loss values\n",
    "            loss_L1 += errG_L1.item()\n",
    "            loss_L2 += errG_L2.item()\n",
    "            loss_gan += errG_GAN.item()\n",
    "\n",
    "            # Calculate PSNR for each generated image and accumulate\n",
    "            psnr = calculate_psnr(generated, frontal)\n",
    "            total_psnr += psnr\n",
    "\n",
    "            # Calculate SSIM for each generated image and accumulate\n",
    "            ssim_val = calculate_ssim(generated, frontal)\n",
    "            total_ssim += ssim_val\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate average PSNR and SSIM for this epoch\n",
    "    avg_psnr = total_psnr / len(train_pipe_loader)\n",
    "    avg_ssim = total_ssim / len(train_pipe_loader)\n",
    "\n",
    "    # Print loss values and metrics\n",
    "    print(f'[{epoch+1}/3] Training absolute losses: L1 {loss_L1/m_train:.7f}; L2 {loss_L2/m_train:.7f}; BCE {loss_gan/m_train:.7f}')\n",
    "    print(f'[{epoch+1}/3] Average PSNR: {avg_psnr:.2f}, Average SSIM: {avg_ssim:.4f}')\n",
    "\n",
    "    # Save images and models\n",
    "    vutils.save_image(profile.data, f'output_RAD/{epoch:03d}_input.jpg', normalize=True)\n",
    "    vutils.save_image(frontal.data, f'output_RAD/{epoch:03d}_real.jpg', normalize=True)\n",
    "    vutils.save_image(generated.data, f'output_RAD/{epoch:03d}_generated.jpg', normalize=True)\n",
    "    torch.save(netG.state_dict(), f'output_RAD/netG_{epoch}.pt')\n",
    "    torch.save(netD.state_dict(), f'output_RAD/netD_{epoch}.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fea59-3379-4305-af73-5c3f0ae61ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e436bc7d-9280-4443-b288-a044fff35fde",
   "metadata": {},
   "source": [
    "**Training with checkpointing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec87bda-1203-4f12-b2ed-78f1fb604b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████| 200/200 [09:18<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Training absolute losses: L1 0.1587943; L2 0.0515081; BCE 0.6418217\n",
      "[1/3] Average PSNR: 15.18, Average SSIM: 0.7265\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  36%|█████████████████████████▉                                              | 72/200 [01:20<04:11,  1.97s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "netG = G().to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = RelativeAvgDiscriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "L1_factor = 0\n",
    "L2_factor = 1\n",
    "GAN_factor = 0.005\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999), eps=1e-8)\n",
    "\n",
    "try:\n",
    "    os.mkdir('output_RAD')\n",
    "except OSError:\n",
    "    pass\n",
    "    \n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Assuming 3 epochs for demonstration\n",
    "    \n",
    "    # Track loss values for each epoch\n",
    "    loss_L1 = 0\n",
    "    loss_L2 = 0\n",
    "    loss_gan = 0\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    \n",
    "    print(\"Starting epoch\", epoch + 1)\n",
    "    with tqdm(total=len(train_pipe_loader), desc=f\"Epoch {epoch}\") as pbar:\n",
    "        for i, data in enumerate(train_pipe_loader, 0):\n",
    "            profile = data[0].view(32, 1, 128, 128).to(device)  # Reshape and move to device\n",
    "            frontal = data[1].view(32, 1, 128, 128).to(device)  # Reshape and move to device\n",
    "\n",
    "            # TRAINING THE DISCRIMINATOR\n",
    "            netD.zero_grad()\n",
    "            optimizerD.zero_grad()\n",
    "\n",
    "            real = Variable(frontal).type('torch.FloatTensor').to(device)\n",
    "            target = Variable(torch.ones(real.size()[0])).to(device)\n",
    "            profile = Variable(profile).type('torch.FloatTensor').to(device)\n",
    "            \n",
    "            real_output = netD(real,real)  # Discriminator output for real images\n",
    "            generated = netG(profile)  # Generate images from profile\n",
    "            fake_output = netD(profile, generated.detach())  # Discriminator output for fake images\n",
    "\n",
    "            # Concatenate real and fake outputs along a new dimension\n",
    "            concatenated = torch.cat((real_output, fake_output), dim=0)\n",
    "\n",
    "            # Create labels for real and fake images\n",
    "            target_real = torch.ones_like(real_output)\n",
    "            target_fake = torch.zeros_like(fake_output)\n",
    "            targets = torch.cat((target_real, target_fake), dim=0)\n",
    "\n",
    "            # Calculate BCE loss for the concatenated outputs\n",
    "            #errD = F.binary_cross_entropy_with_logits(concatenated, targets)\n",
    "\n",
    "            errD = criterion(concatenated, targets.float())\n",
    "            errD.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # TRAINING THE GENERATOR\n",
    "            netG.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "            generated = netG(profile)\n",
    "            output = netD(profile, generated)\n",
    "\n",
    "            # G wants to have the synthetic images be accepted by D\n",
    "            errG_GAN = criterion(output, torch.ones_like(output).float())\n",
    "\n",
    "            # Calculate L1 and L2 loss between generated and real images\n",
    "            errG_L1 = F.l1_loss(generated, frontal.float())\n",
    "            errG_L2 = F.mse_loss(generated, frontal.float())\n",
    "           \n",
    "            #errG_L1 = torch.mean(torch.abs(real - generated))\n",
    "            #errG_L2 = torch.mean(torch.pow((real - generated), 2))\n",
    "            \n",
    "            # Total generator loss\n",
    "            errG = GAN_factor * errG_GAN + L1_factor * errG_L1 + L2_factor * errG_L2\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Update loss values\n",
    "            loss_L1 += errG_L1.item()\n",
    "            loss_L2 += errG_L2.item()\n",
    "            loss_gan += errG_GAN.item()\n",
    "\n",
    "            # Calculate PSNR for each generated image and accumulate\n",
    "            psnr = calculate_psnr(generated, frontal)\n",
    "            total_psnr += psnr\n",
    "\n",
    "            # Calculate SSIM for each generated image and accumulate\n",
    "            ssim_val = calculate_ssim(generated, frontal)\n",
    "            total_ssim += ssim_val\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate average PSNR and SSIM for this epoch\n",
    "    avg_psnr = total_psnr / len(train_pipe_loader)\n",
    "    avg_ssim = total_ssim / len(train_pipe_loader)\n",
    "\n",
    "      # Save checkpoint after each epoch\n",
    "    checkpoint_state = {\n",
    "      'epoch': epoch,\n",
    "      'netG_state_dict': netG.state_dict(),\n",
    "      'netD_state_dict': netD.state_dict(),\n",
    "      'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "      'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "      'loss_L1': loss_L1,\n",
    "      'loss_L2': loss_L2,\n",
    "      'loss_gan': loss_gan,\n",
    "      'psnr_values': psnr_values,\n",
    "      'ssim_values': ssim_values,\n",
    "      #'losses_L1': losses_L1,\n",
    "      #'losses_L2': losses_L2,\n",
    "      #'losses_gan': losses_gan,\n",
    "    }\n",
    "    torch.save(checkpoint_state, os.path.join(checkpoint_dir, f\"checkpoint_{epoch}.pth\"))\n",
    "\n",
    "    # Print loss values and metrics\n",
    "    print(f'[{epoch+1}/3] Training absolute losses: L1 {loss_L1/m_train:.7f}; L2 {loss_L2/m_train:.7f}; BCE {loss_gan/m_train:.7f}')\n",
    "    print(f'[{epoch+1}/3] Average PSNR: {avg_psnr:.2f}, Average SSIM: {avg_ssim:.4f}')\n",
    "\n",
    "    # Save images and models\n",
    "    vutils.save_image(profile.data, f'output_RAD/{epoch:03d}_input.jpg', normalize=True)\n",
    "    vutils.save_image(frontal.data, f'output_RAD/{epoch:03d}_real.jpg', normalize=True)\n",
    "    vutils.save_image(generated.data, f'output_RAD/{epoch:03d}_generated.jpg', normalize=True)\n",
    "    torch.save(netG.state_dict(), f'output_RAD/netG_{epoch}.pt')\n",
    "    torch.save(netD.state_dict(), f'output_RAD/netD_{epoch}.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f8ef9-ff28-4eaa-9518-36fc919a12f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f4218a7-fa3e-4525-9096-d2760070884c",
   "metadata": {},
   "source": [
    "**Loading checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e81f3d-b63f-4efa-8b45-e22205579de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "110135c7-b669-4cf9-9b52-fae28ad742a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████| 200/200 [09:20<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/3] Training absolute losses: L1 0.0356334; L2 0.0025022; BCE 0.7002112\n",
      "[4/3] Average PSNR: 26.07, Average SSIM: 0.9635\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████████████| 200/200 [09:18<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/3] Training absolute losses: L1 0.0318828; L2 0.0019760; BCE 0.7046735\n",
      "[5/3] Average PSNR: 27.08, Average SSIM: 0.9714\n"
     ]
    }
   ],
   "source": [
    "latest_epoch = 2\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_{latest_epoch}.pth\")\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load model and optimizer states\n",
    "netG.load_state_dict(checkpoint['netG_state_dict'])\n",
    "netD.load_state_dict(checkpoint['netD_state_dict'])\n",
    "optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "optimizerD.load_state_dict(checkpoint['optimizerD_state_dict'])\n",
    "\n",
    "# Load training progress\n",
    "loss_L1 = checkpoint['loss_L1']\n",
    "loss_L2 = checkpoint['loss_L2']\n",
    "loss_gan = checkpoint['loss_gan']\n",
    "psnr_values = checkpoint['psnr_values']\n",
    "ssim_values = checkpoint['ssim_values']\n",
    "#losses_L1 = checkpoint['losses_L1']\n",
    "#losses_L2 = checkpoint['losses_L2']\n",
    "#losses_gan = checkpoint['losses_gan']\n",
    "\n",
    "# Start training from the loaded epoch\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "# Let's train for 30 epochs (meaning, we go through the entire training set 30 times):\n",
    "for epoch in range(start_epoch,5):\n",
    "   # Track loss values for each epoch\n",
    "    loss_L1 = 0\n",
    "    loss_L2 = 0\n",
    "    loss_gan = 0\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    \n",
    "    print(\"Starting epoch\", epoch + 1)\n",
    "    with tqdm(total=len(train_pipe_loader), desc=f\"Epoch {epoch}\") as pbar:\n",
    "        for i, data in enumerate(train_pipe_loader, 0):\n",
    "            profile = data[0].view(32, 1, 128, 128).to(device)  # Reshape and move to device\n",
    "            frontal = data[1].view(32, 1, 128, 128).to(device)  # Reshape and move to device\n",
    "\n",
    "            # TRAINING THE DISCRIMINATOR\n",
    "            netD.zero_grad()\n",
    "            optimizerD.zero_grad()\n",
    "\n",
    "            real = Variable(frontal).type('torch.FloatTensor').to(device)\n",
    "            target = Variable(torch.ones(real.size()[0])).to(device)\n",
    "            profile = Variable(profile).type('torch.FloatTensor').to(device)\n",
    "            \n",
    "            real_output = netD(real,real)  # Discriminator output for real images\n",
    "            generated = netG(profile)  # Generate images from profile\n",
    "            fake_output = netD(profile, generated.detach())  # Discriminator output for fake images\n",
    "\n",
    "            # Concatenate real and fake outputs along a new dimension\n",
    "            concatenated = torch.cat((real_output, fake_output), dim=0)\n",
    "\n",
    "            # Create labels for real and fake images\n",
    "            target_real = torch.ones_like(real_output)\n",
    "            target_fake = torch.zeros_like(fake_output)\n",
    "            targets = torch.cat((target_real, target_fake), dim=0)\n",
    "\n",
    "            # Calculate BCE loss for the concatenated outputs\n",
    "            #errD = F.binary_cross_entropy_with_logits(concatenated, targets)\n",
    "\n",
    "            errD = criterion(concatenated, targets.float())\n",
    "            errD.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # TRAINING THE GENERATOR\n",
    "            netG.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "            generated = netG(profile)\n",
    "            output = netD(profile, generated)\n",
    "\n",
    "            # G wants to have the synthetic images be accepted by D\n",
    "            errG_GAN = criterion(output, torch.ones_like(output).float())\n",
    "\n",
    "            # Calculate L1 and L2 loss between generated and real images\n",
    "            errG_L1 = F.l1_loss(generated, frontal.float())\n",
    "            errG_L2 = F.mse_loss(generated, frontal.float())\n",
    "           \n",
    "            #errG_L1 = torch.mean(torch.abs(real - generated))\n",
    "            #errG_L2 = torch.mean(torch.pow((real - generated), 2))\n",
    "            \n",
    "            # Total generator loss\n",
    "            errG = GAN_factor * errG_GAN + L1_factor * errG_L1 + L2_factor * errG_L2\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Update loss values\n",
    "            loss_L1 += errG_L1.item()\n",
    "            loss_L2 += errG_L2.item()\n",
    "            loss_gan += errG_GAN.item()\n",
    "\n",
    "            # Calculate PSNR for each generated image and accumulate\n",
    "            psnr = calculate_psnr(generated, frontal)\n",
    "            total_psnr += psnr\n",
    "\n",
    "            # Calculate SSIM for each generated image and accumulate\n",
    "            ssim_val = calculate_ssim(generated, frontal)\n",
    "            total_ssim += ssim_val\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate average PSNR and SSIM for this epoch\n",
    "    avg_psnr = total_psnr / len(train_pipe_loader)\n",
    "    avg_ssim = total_ssim / len(train_pipe_loader)\n",
    "\n",
    "      # Save checkpoint after each epoch\n",
    "    checkpoint_state = {\n",
    "      'epoch': epoch,\n",
    "      'netG_state_dict': netG.state_dict(),\n",
    "      'netD_state_dict': netD.state_dict(),\n",
    "      'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "      'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "      'loss_L1': loss_L1,\n",
    "      'loss_L2': loss_L2,\n",
    "      'loss_gan': loss_gan,\n",
    "      'psnr_values': psnr_values,\n",
    "      'ssim_values': ssim_values,\n",
    "      #'losses_L1': losses_L1,\n",
    "      #'losses_L2': losses_L2,\n",
    "      #'losses_gan': losses_gan,\n",
    "    }\n",
    "    torch.save(checkpoint_state, os.path.join(checkpoint_dir, f\"checkpoint_{epoch}.pth\"))\n",
    "\n",
    "    # Print loss values and metrics\n",
    "    print(f'[{epoch+1}/3] Training absolute losses: L1 {loss_L1/m_train:.7f}; L2 {loss_L2/m_train:.7f}; BCE {loss_gan/m_train:.7f}')\n",
    "    print(f'[{epoch+1}/3] Average PSNR: {avg_psnr:.2f}, Average SSIM: {avg_ssim:.4f}')\n",
    "\n",
    "    # Save images and models\n",
    "    vutils.save_image(profile.data, f'output_RAD/{epoch:03d}_input.jpg', normalize=True)\n",
    "    vutils.save_image(frontal.data, f'output_RAD/{epoch:03d}_real.jpg', normalize=True)\n",
    "    vutils.save_image(generated.data, f'output_RAD/{epoch:03d}_generated.jpg', normalize=True)\n",
    "    torch.save(netG.state_dict(), f'output_RAD/netG_{epoch}.pt')\n",
    "    torch.save(netD.state_dict(), f'output_RAD/netD_{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed6338-0e2d-4874-a5a5-59f5e4fdc612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
